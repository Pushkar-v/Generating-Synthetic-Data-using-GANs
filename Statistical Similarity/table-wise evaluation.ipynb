{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Transforming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "real_data = pd.read_csv('real_ctdata.csv')\n",
    "syn_data = pd.read_csv('ctgan_synthetic_382.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Drop columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = real_data.loc[:, ~real_data.columns.str.contains('^Unnamed|id|ID')]\n",
    "syn_data = syn_data.loc[:, ~syn_data.columns.str.contains('^Unnamed|ID|id')]\n",
    "real_data = real_data.dropna()\n",
    "syn_data = syn_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encode categorical columns and normalize numerical columns (if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical columns that are string types\n",
    "cat_list = ['ADMISSION_TYPE', 'INSURANCE', 'ETHNICITY', 'GENDER']\n",
    "# Categorical columns that are numerical types\n",
    "numcat_list = [ 'HOSPITAL_EXPIRE_FLAG','EXPIRE_FLAG']\n",
    "# Numerical columns\n",
    "num_list = ['LOS','certain conditions originating in the perinatal period',\n",
    "       'complications of pregnancy, childbirth, and the puerperium',\n",
    "       'congenital anomalies',\n",
    "       'diseases of the blood and blood-forming organs',\n",
    "       'diseases of the circulatory system',\n",
    "       'diseases of the digestive system',\n",
    "       'diseases of the genitourinary system',\n",
    "       'diseases of the musculoskeletal system and connective tissue',\n",
    "       'diseases of the nervous system', 'diseases of the respiratory system',\n",
    "       'diseases of the sense organs',\n",
    "       'diseases of the skin and subcutaneous tissue',\n",
    "       'endocrine, nutritional and metabolic diseases, and immunity disorders',\n",
    "       'external causes of injury and supplemental classification',\n",
    "       'infectious and parasitic diseases', 'injury and poisoning',\n",
    "       'mental disorders', 'neoplasms',\n",
    "       'symptoms, signs, and ill-defined conditions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "le = LabelEncoder()\n",
    "oe = OneHotEncoder(sparse=False)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "def transform_cat(df):\n",
    "    df_2 = df.apply(le.fit_transform)\n",
    "    df_oe = oe.fit_transform(df_2)\n",
    "    df_oe = pd.DataFrame(df_oe)\n",
    "    return df_oe\n",
    "\n",
    "def transform_numcat(df):\n",
    "    df_oe = oe.fit_transform(df)\n",
    "    df_oe = pd.DataFrame(df_oe)\n",
    "    return df_oe\n",
    "\n",
    "def transform_num(df):\n",
    "    df_2 = scaler.fit_transform(df)\n",
    "    df_2 = pd.DataFrame(df_2)\n",
    "    return df_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olivi\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\olivi\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\olivi\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\olivi\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "numcat_train = transform_numcat(real_data[numcat_list])\n",
    "num_train = transform_num(real_data[num_list])\n",
    "cat_train = transform_cat(real_data[cat_list])\n",
    "\n",
    "numcat_test = transform_numcat(syn_data[numcat_list])\n",
    "num_test = transform_num(syn_data[num_list])\n",
    "cat_test = transform_cat(syn_data[cat_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93083, 151)\n",
      "(93000, 113)\n"
     ]
    }
   ],
   "source": [
    "# Integrate datasets\n",
    "x_train = pd.concat([numcat_test, cat_train], axis=1, sort=False)\n",
    "x_train = pd.concat([x_train, num_train], axis=1, sort=False)\n",
    "\n",
    "x_test = pd.concat([numcat_test, cat_test], axis=1, sort=False)\n",
    "x_test = pd.concat([x_test, num_test], axis=1, sort=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reshape for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "# Flatten the data into vectors\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder Structure Building\n",
    "\n",
    "A single fully-connected neural layer as encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "def modeling_autoencoder(latent_dim, x_train):\n",
    "    original_dim= x_train.shape[1]\n",
    "\n",
    "    # this is our input placeholder\n",
    "    input_data = Input(shape=(original_dim,))\n",
    "    # \"encoded\" is the encoded representation of the input\n",
    "    encoded = Dense(latent_dim, activation='relu')(input_data)\n",
    "    # \"decoded\" is the lossy reconstruction of the input\n",
    "    decoded = Dense(original_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "    # this model maps an input to its reconstruction (Define a model that would turn input_data into decoded output)\n",
    "    autoencoder = Model(input_data, decoded)\n",
    "    \n",
    "    #### Create a separate encoder model ####\n",
    "    # this model maps an input to its encoded representation\n",
    "    encoder = Model(input_data, encoded)\n",
    "    \n",
    "    #### as well as the decoder model ####\n",
    "    # create a placeholder for an encoded (assigned # of dimensions) input\n",
    "    encoded_input = Input(shape=(latent_dim,))\n",
    "    # retrieve the last layer of the autoencoder model\n",
    "    decoder_layer = autoencoder.layers[-1]\n",
    "    # create the decoder model\n",
    "    decoder = Model(encoded_input, decoder_layer(encoded_input)) \n",
    "    \n",
    "    #### Autoencoder model training ####\n",
    "    autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "    \n",
    "    autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_split = 0.2)\n",
    "    \n",
    "    return encoder, decoder\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_encoder = modeling_autoencoder(1, x_train)[0]\n",
    "encoded_testdata = trained_encoder.predict(x_test)\n",
    "encoded_traindata = trained_encoder.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decide the bins by yourself:\n",
    "# The upper bound should be 2 more steps more than the maximum value of both vectors\n",
    "# Controling the whole length of the bins to around 200 woyld be optimal \n",
    "\n",
    "bins = np.arange(0,2100,20)\n",
    "\n",
    "real_inds = pd.DataFrame(np.digitize(encoded_traindata, bins), columns = ['inds'])\n",
    "syn_inds = pd.DataFrame(np.digitize(encoded_testdata, bins), columns = ['inds'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_probs(table,column):\n",
    "    counts = table[column].value_counts()\n",
    "    freqs = {counts.index[i]: counts.values[i] for i in range(len(counts.index))}\n",
    "    for i in range(1, len(bins)+1):\n",
    "        if i not in freqs.keys():\n",
    "            freqs[i] = 0\n",
    "    sorted_freqs = {}\n",
    "    for k in sorted(freqs.keys()):\n",
    "        sorted_freqs[k] = freqs[k]\n",
    "    probs = []\n",
    "    for k,v in sorted_freqs.items():\n",
    "        probs.append(v/len(table[column]))\n",
    "    return sorted_freqs, np.array(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "real_p = identify_probs(real_inds,'inds')[1]\n",
    "syn_p = identify_probs(syn_inds,'inds')[1]\n",
    "def cos_similarity(p,q):\n",
    "    return 1 - distance.cosine(p, q)\n",
    "cos_similarity(real_p,syn_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract 5-dimensional data from autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_encoder = modeling_autoencoder(5, x_train)[0]\n",
    "encoded_testdata = trained_encoder.predict(x_test)\n",
    "encoded_traindata = trained_encoder.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# T-sne visualization\n",
    "pca = PCA(n_components=2, random_state = 0)\n",
    "pca_train = pca.fit_transform(encoded_traindata)\n",
    "pca_test = pca.fit_transform(encoded_testdata)\n",
    "pca_train_df = pd.DataFrame(data = pca_train, columns = ('Dim_1','Dim_2'))\n",
    "pca_test_df = pd.DataFrame(data = pca_test, columns = ('Dim_1','Dim_2'))\n",
    "\n",
    "plt.figure(figsize = [14, 5])\n",
    "plt.subplot(121)\n",
    "plt.title('Original dataset')\n",
    "plt.scatter(pca_train_df['Dim_1'],pca_train_df['Dim_2'], marker = 'o')\n",
    "plt.xlabel('Dimension 1',fontsize=14)\n",
    "plt.ylabel('Dimension 2',fontsize=14)\n",
    "# plt.axis([-1.0, 2.0, -0.5, 1.5]) \n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Synthetic dataset')\n",
    "plt.scatter(pca_test_df['Dim_1'],pca_test_df['Dim_2'], marker = 'o')\n",
    "plt.xlabel('Dimension 1',fontsize=14)\n",
    "plt.ylabel('Dimension 2',fontsize=14)\n",
    "# plt.axis([-1.0, 2.0, -0.5, 1.5])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# T-sne visualization\n",
    "tsne = TSNE(n_components = 2, random_state = 0)\n",
    "tsne_train = tsne.fit_transform(encoded_traindata)\n",
    "tsne_test = tsne.fit_transform(encoded_testdata)\n",
    "tsne_train_df = pd.DataFrame(data = tsne_train, columns = ('Dim_1','Dim_2'))\n",
    "tsne_test_df = pd.DataFrame(data = tsne_test, columns = ('Dim_1','Dim_2'))\n",
    "\n",
    "plt.figure(figsize = [14, 5])\n",
    "plt.subplot(121)\n",
    "plt.title('Original dataset')\n",
    "plt.scatter(tsne_train_df['Dim_1'],tsne_train_df['Dim_2'], marker = 'o')\n",
    "plt.xlabel('Dimension 1',fontsize=14)\n",
    "plt.ylabel('Dimension 2',fontsize=14)\n",
    "# plt.axis([-30, 40, -40, 40])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Synthetic dataset')\n",
    "plt.scatter(tsne_test_df['Dim_1'],tsne_test_df['Dim_2'], marker = 'o')\n",
    "plt.xlabel('Dimension 1',fontsize=14)\n",
    "plt.ylabel('Dimension 2',fontsize=14)\n",
    "# plt.axis([-30, 40, -40, 40])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
